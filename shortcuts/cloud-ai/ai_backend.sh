


# AI LLM use often does not require more than the basic understanding or allusion conveyed by documentation. Some loss of detail and less likely, accuracy, may be a very acceptable tradeoff.
#  Particularly, processing of quoting, escaping, etc, through JSON, through 'jq', etc, may be an absolutely unacceptable concern to avoid by filtering.
_ai_filter() {
    # Delete control characters, delete carriage returns (leaving UNIX only line endings), delete risky input characters.
    # Then, translate common but unnecessary unicode characters.
    # Last, delete all control characters outside the allowlist.
    LC_ALL=C tr -d '\000-\010\013\014\016-\037\177' | \
    LC_ALL=C tr -d '\r' | \
    LC_ALL=C tr '"<>()?:;[]{}\\*&'"'" '_' | LC_ALL=C tr '\042\047\050\051\077\072\073\133\135\173\175\134\052\046' '_' | \
    LC_ALL=C perl -CS -pe '
        s/[\x{2010}-\x{2015}\x{2212}]/-/g;                 # dashes/minus -> -
        s/\x{00D7}/x/g;                                    # multiplication sign -> x
        s/[\x{00A0}\x{2000}-\x{200A}\x{202F}\x{205F}\x{3000}]/ /g;   # wide/no-break spaces -> space
        s/[\x{200B}-\x{200D}\x{FEFF}]//g;                  # zero-width/BOM -> delete
        s/[\x{202A}-\x{202E}\x{2066}-\x{2069}]//g;         # bidi controls -> delete
        s/[\x{2028}\x{2029}]/\n/g;                         # line/para separators -> newline
        s/\x{2026}/.../g;                                  # ellipsis
        s/[\x{2022}\x{00B7}]/-/g;                          # bullets/dots -> -
    ' | \
    LC_ALL=C tr -c '~A-Za-z0-9 .,_+/\\\-=\n\t' '_' | LC_ALL=C tr -dc '~A-Za-z0-9 .,_+/\\\-=\n\t'
}




_ai_backend_askGibberish_semanticAssist() {

    local currentAImodel
    local currentAIprovider

    if [[ "$2" == "openrouter" ]]
    then
        currentAImodel='models: [ "nvidia/nemotron-3-nano-30b-a3b", "openai/gpt-oss-120b" ], reasoning: { exclude: true }, provider: { "sort": "throughput" }'
        currentAIprovider="openrouter"

        #inference...
        #return 0
    fi

    if [[ "$2" == "ollama" ]]
    then
        currentAImodel='model: "Nemotron-3-Nano-30B-A3B-256k-virtuoso", think:true'
        currentAIprovider="ollama"

        #inference...
        #return 0
    fi

    [[ "$currentAImodel" == "" ]] && return 1


    local current_gibberish="gibberish"


cat | {
echo '~~~'
cat
echo '~~~'

cat << CZXWXcRMTo8EmM8i4d

Should be AI autogenerated keywords here, intended to summarize concept from code for keyword search. Are these valid keywords, or did the AI LLM model apparently begin outputting gibberish?

Keywords 'empty', 'blank', etc, for situations not applicable to search terms, may be valid.

Contradictory keywords such as 'empty', 'blank', 'lack', etc, with 'terminal' and 'codeblock' are gibberish.

Always err on the side of assuming the output is gibberish. Typos and misspellings are gibberish.

If there is a phrase 'here are the keywords for the code', or similar, that is gibberish.

If there is anything a reasonable person might be at least slightly offended by, that is gibberish.

Please only output one word gibberish or valid. Do not output any other statements. Response will be processed automatically, so the one word answer either gibberish or valid will be helpful, any other output will be unhelpful.

CZXWXcRMTo8EmM8i4d
} | inference_cache_dir="" _ai_backend_procedure "$currentAImodel" "$currentAIprovider" "_ai_backend_ask_noop" "_ai_backend_ask_noop" | grep -i '^[[:space:]]*valid' >/dev/null 2>&1 && current_gibberish="valid"

    if [[ "$current_gibberish" == "gibberish" ]]
    then
        return 1
    fi


    return 0
}






_ai_backend_askGibberish() {

    local currentAImodel
    local currentAIprovider

    if [[ "$2" == "openrouter" ]]
    then
        currentAImodel='models: [ "nvidia/nemotron-3-nano-30b-a3b", "openai/gpt-oss-120b" ], reasoning: { exclude: true }, provider: { "sort": "throughput" }'
        currentAIprovider="openrouter"

        #inference...
        #return 0
    fi

    if [[ "$2" == "ollama" ]]
    then
        currentAImodel='model: "Nemotron-3-Nano-30B-A3B-256k-virtuoso", think:true'
        currentAIprovider="ollama"

        #inference...
        #return 0
    fi

    [[ "$currentAImodel" == "" ]] && return 1


    local current_gibberish="gibberish"


cat | {
echo '~~~'
cat
echo '~~~'

cat << CZXWXcRMTo8EmM8i4d


Should be AI autogenerated text.

Information, code, commands, etc, that are correct, is valid.

Somewhat random seeming letters, numbers, etc, may be valid if brief. If plausible, such as an address, definitely valid.

A string of random alphanumeric characters less than 56 characters is considered brief, is often a unique identifier, definitely valid.

Keywords 'empty', 'blank', 'fail', etc, for infeasible situations, may be valid.

Contradictory keywords such as 'empty', 'blank', 'lack', etc, with 'terminal' and 'codeblock' are gibberish.

Always err on the side of assuming the output is gibberish. Typos and misspellings are gibberish.

Non-portable, unreliable, commands and parameters are gibberish.

Dangerous bash shellcode commands such as rm , passwd , etc , are gibberish.

Semicolon characters, etc, which may escape to create a backdoor or other harm, are suspicious of being gibberish. If semicolon or other command escape characters are followed by a command that may be used for good but is often associated with bad such as an interactive shell terminal, reading files, or network listening, etc, assume the worst, this is gibberish.

If there is a phrase 'here is the code', or similar, that is gibberish.

If there is anything a reasonable person might be at least slightly offended by, that is gibberish.

Suggesting dangerous commands such as rm is a bannable offense in many communities, dangerous commands are gibberish.

If the given information seems valid, say 'valid'. Otherwise, say 'gibberish'. If the given information follows a recognizable format, do not summarize, only output the word 'valid' in that case.

Please only output one word gibberish or valid. Do not output any other statements. Response will be processed automatically, so the one word answer either gibberish or valid will be helpful, any other output will be unhelpful.


CZXWXcRMTo8EmM8i4d
} | inference_cache_dir="" _ai_backend_procedure "$currentAImodel" "$currentAIprovider" "_ai_backend_ask_noop" "_ai_backend_ask_noop" | grep -i '^[[:space:]]*valid' >/dev/null 2>&1 && current_gibberish="valid"

    if [[ "$current_gibberish" == "gibberish" ]]
    then
        return 1
    fi


    return 0
}




_ai_backend_askPolite() {

    local currentAImodel
    local currentAIprovider

    if [[ "$2" == "openrouter" ]]
    then
        currentAImodel='models: [ "nvidia/nemotron-3-nano-30b-a3b", "openai/gpt-oss-120b" ], reasoning: { exclude: true }, provider: { "sort": "throughput" }'
        currentAIprovider="openrouter"

        #inference...
        #return 0
    fi

    if [[ "$2" == "ollama" ]]
    then
        currentAImodel='model: "Nemotron-3-Nano-30B-A3B-256k-virtuoso", think:true'
        currentAIprovider="ollama"

        #inference...
        #return 0
    fi

    [[ "$currentAImodel" == "" ]] && return 1


    local current_polite="offended"


cat | {
echo '~~~'
cat
echo '~~~'

cat << CZXWXcRMTo8EmM8i4d

Might a reasonable person be at least slightly offended by this preceding text? Output only offended or safe, one word answer is needed for automation, one word offended or safe will be helpful, any other output will be unhelpful.

CZXWXcRMTo8EmM8i4d
} | inference_cache_dir="" _ai_backend_procedure "$currentAImodel" "$currentAIprovider" "_ai_backend_ask_noop" "_ai_backend_ask_noop" | grep -i '^[[:space:]]*safe' >/dev/null 2>&1 && current_polite="safe"

    if [[ "$current_polite" == "offended" ]]
    then
        return 1
    fi


    return 0
}


_ai_backend_ask_noop() {
    cat > /dev/null
}



# WARNING: CAUTION: DANGER: Unlike most other 'ubiquitous_bash' functions, this does NOT provide full abstraction. Calling this with a pattern - dispatch, gibberish detection, etc, with dedicated "$safeTmp" files, is STRICTLY NECESSARY.
#
# WARNING: Calling the AI model once (writing to /dev/null) may be necessary to ensure (eg. ollama) inference server is ready.
#
# WARNING: Gibberish and offensive content detection are STRICTLY NECESSARY regardless of whether the model supposedly has a 'safety' layer. Processing very large amounts of content WILL result in some offensive gibberish.
#
# ATTENTION: WARNING: It is necessary to reuse a "$safeTmp" directory, as creating new instances can be prohibitively slow (especially, but not limited to MSWindows/Cygwin - UNIX/Linux can also be too slow in this situation).
#
#find "$1" -type f -name '*.sh' -print0 | xargs -0 -x -L 1 -P 1 bash -c '"'"$scriptAbsoluteLocation"'"'' --embed _semanticAssist_bash_procedure "$@"' _
#
_ai_backend_procedure() {
    # 'model: "meta-llama/llama-3.1-405b-instruct", provider: { "order": ["Fireworks"], "sort": "throughput" }'
    # 'model: "meta-llama/llama-3.1-405b-instruct"'
    # 'model: "Llama-3-augment"''
    local currentAImodel="$1"
    [[ "$currentAImodel" == "" ]] && currentAImodel='model: "Nemotron-3-Nano-30B-A3B-256k-virtuoso", think:true'

    local currentAIprovider="$2"
    [[ "$currentAIprovider" == "" ]] && currentAIprovider="ollama"

    local askGibberish="$3"
    [[ "$askGibberish" == "" ]] && askGibberish="_ai_backend_askGibberish"

    local askPolite="$4"
    [[ "$askPolite" == "" ]] && askPolite="_ai_backend_askPolite"


    
    local currentMaxTime="$5"
    [[ "$currentMaxTime" == "" ]] && currentMaxTime="180"

    local current_keepalive_time="$6"
    [[ "$current_keepalive_time" == "" ]] && current_keepalive_time="300"



    local current_sub_sessionid_ai_backend=$(_uid 28)
    local current_sub_safeTmp_ai_backend="$safeTmp"/ai_backend_"$current_sub_sessionid_ai_backend"
    if ! mkdir -p "$current_sub_safeTmp_ai_backend"
    then
        # Outputting an error message, even to stderr, may get redirected to the inference output, causing bad breakage, confusion, etc.
        #( _messageERROR 'FAIL: mkdir: $current_sub_safeTmp_ai_backend' >&2 )
        _stop 1
    fi

    cat > "$current_sub_safeTmp_ai_backend"/_input.txt
    cat "$current_sub_safeTmp_ai_backend"/_input.txt | _ai_filter > "$current_sub_safeTmp_ai_backend"/_safe_input.txt

    cat "$current_sub_safeTmp_ai_backend"/_input.txt | sha512sum | head -c 128 | tr 'A-Z' 'a-z' | tr -dc 'a-z0-9' > "$current_sub_safeTmp_ai_backend"/_input_hash.txt




    #export inference_cache_dir="$current_output_dir"/inference_cache/
    #
    # Only cache if response is <2k compressed base64 .
    # HASHHASHHASH.tmp -> compress response -> add newline -> append to raw flat file
    # find/retrieve relevant line using hash - grep, etc - then decompress and use the cached result
    #
    # input prompt is not cached - only output response is cached - hash is sufficient to 'compress' and identify the input prompt
    #
    # Appending to raw flat file will require file locking.
    # Due to the low stakes, if the lock file does not contain the calling sessionid, etc, writing the cache should simply be abandoned.
    #
    # newline is an important out-of-band indicator - grep should only search for the hash beginning on a new line... attempts to get the output to include hashes matching some input hashes... should not cause reading the wrong response from cache
    if [[ "$inference_cache_dir" != "" ]] && [[ -e "$inference_cache_dir"/inference_cache_sha512_xz.txt ]]
    then
        if grep -m1 '^'$(cat "$current_sub_safeTmp_ai_backend"/_input_hash.txt) "$inference_cache_dir"/inference_cache_sha512_xz.txt | tail -c +129 | base64 -d | xz -d -C sha256 > "$current_sub_safeTmp_ai_backend"/_output.txt 2>/dev/null
        then
            cat "$current_sub_safeTmp_ai_backend"/_output.txt
            _safeRMR "$current_sub_safeTmp_ai_backend"
            return 0
        else
            rm -f "$current_sub_safeTmp_ai_backend"/_output.txt
        fi
    fi

	local current_OLLAMA_HOST="$OLLAMA_HOST"
    [[ "$current_OLLAMA_HOST" == "" ]] && current_OLLAMA_HOST='127.0.0.1:11434'

    # Iterations to attempt a non-gibberish, non-offensive, response, may be as high as 45 for high-stakes narrow purpose use cases (eg. 'augment'), or lower than 25 for merely generating documentation annotations.
    local currentNetworkIteration
    local currentIteration=0
    local current_good_output="false"
    while [[ "$current_good_output" != "true" ]] && [[ "$currentIteration" -lt 25 ]]
    do
            # TODO: WIP!
            # Fake inference call for testing.
            #echo "$RANDOM""$RANDOM""$RANDOM""$RANDOM""$RANDOM" > "$current_sub_safeTmp_ai_backend"/_output.txt

            rm -f "$current_sub_safeTmp_ai_backend"/_output.txt

            if [[ "$currentAIprovider" == "ollama" ]]
            then
                currentNetworkIteration=0
                while [[ "$currentNetworkIteration" -le 20 ]] && ! cat "$current_sub_safeTmp_ai_backend"/_safe_input.txt | jq -Rs '{'"$currentAImodel"', prompt:., stream: false}' | curl -fsS --max-time "$currentMaxTime" -X POST -H "Content-Type: application/json" --data-binary @- http://localhost:11434/api/generate | jq -er '.response' > "$current_sub_safeTmp_ai_backend"/_output_unsafe.txt
                do
                    sleep 6
                    currentNetworkIteration=$(( currentNetworkIteration + 1 ))
                done
            fi

            if [[ "$currentAIprovider" == "openrouter" ]]
            then
                currentNetworkIteration=0
                while [[ "$currentNetworkIteration" -le 32 ]] && ! cat "$current_sub_safeTmp_ai_backend"/_safe_input.txt | jq -Rs '{'"$currentAImodel"', messages: [{"role": "user", "content": .}] }' | curl -fsS --max-time "$currentMaxTime" --keepalive-time "$current_keepalive_time" --compressed --tcp-fastopen --http2 -X POST https://openrouter.ai/api/v1/chat/completions -H "Content-Type: application/json" -H "Authorization: Bearer $OPENROUTER_API_KEY" --data-binary @- | jq -er '.choices[0].message.content' > "$current_sub_safeTmp_ai_backend"/_output_unsafe.txt
                do
                    [[ "$currentNetworkIteration" -le 25 ]] && sleep $(( ( "$currentNetworkIteration" - 20 ) ** 4 ))
                    [[ "$currentNetworkIteration" -gt 23 ]] && sleep $(( "$RANDOM" % 128 ))
                    [[ "$currentNetworkIteration" -gt 28 ]] && sleep $(( "$RANDOM" % 128 ))
                    # Optional. Appropriate for very large unattended batch jobs. Waits hours, potentially long enough for service provider outages, etc.
                    #[[ "$currentNetworkIteration" -gt 28 ]] && sleep 5400
                    currentNetworkIteration=$(( currentNetworkIteration + 1 ))
                done
            fi

            cat "$current_sub_safeTmp_ai_backend"/_output_unsafe.txt | sed -E -e '/^<(think|analysis)>/! s@^[^<]*</(think|analysis)>[[:space:]]*@@' -e 's@^<(think|analysis)>[^<]*</\1>[[:space:]]*@@' -e 's@^(analysis|thought|thinking|deliberate):[[:space:]]*@@' | _ai_filter > "$current_sub_safeTmp_ai_backend"/_output.txt
            rm -f "$current_sub_safeTmp_ai_backend"/_output_unsafe.txt
            #"$current_sub_safeTmp_ai_backend"/_output.txt

            #sed -E -e 's@^<(think|analysis)>[^<]*</\1>[[:space:]]*@@' -e 's@^(analysis|thought|thinking|deliberate):[[:space:]]*@@' | _ai_filter

            [[ -e "$current_sub_safeTmp_ai_backend"/_output.txt ]] && cat "$current_sub_safeTmp_ai_backend"/_output.txt | "$askGibberish" "$currentAImodel" "$currentAIprovider" > /dev/null && cat "$current_sub_safeTmp_ai_backend"/_output.txt | "$askPolite" "$currentAImodel" "$currentAIprovider" > /dev/null && current_good_output="true"
    done
    if [[ "$current_good_output" != "true" ]] || [[ ! -e "$current_sub_safeTmp_ai_backend"/_output.txt ]]
    then    
        _safeRMR "$current_sub_safeTmp_ai_backend"
        return 1
    fi






    # WARNING: Locking mechanism need not be perfect. Occasionally missing the opportunity to write to cache is less important than minimizing loss of throughput.
    if [[ "$inference_cache_dir" != "" ]]
    then
        mkdir -p "$inference_cache_dir"

        local currentIteration=0
        while [[ "$currentIteration" -lt 1 ]] && ls -1 "$inference_cache_dir"/*.lock > /dev/null 2>&1
        do
            sleep 1
            currentIteration=$(( currentIteration + 1 ))
        done

        if [[ "$currentIteration" -ge 1 ]]
        then
            # Occasionally delete all locks, in case any are not getting deleted appropriately.
            [[ $(( "$RANDOM"%1000 )) == "0" ]] && rm -f "$inference_cache_dir"/*.lock
            cat "$current_sub_safeTmp_ai_backend"/_output.txt
            rm -f "$current_sub_safeTmp_ai_backend"/_output.txt
            _safeRMR "$current_sub_safeTmp_ai_backend"
            return 0
        fi

        echo "$current_sub_sessionid_ai_backend" > "$inference_cache_dir"/"$current_sub_sessionid_ai_backend".lock

        cp -f "$inference_cache_dir"/inference_cache_sha512_xz.txt "$inference_cache_dir"/inference_cache_sha512_xz.txt.tmp-"$current_sub_sessionid_ai_backend".txt 2> /dev/null
        echo '' >> "$inference_cache_dir"/inference_cache_sha512_xz.txt.tmp-"$current_sub_sessionid_ai_backend".txt
        cat "$current_sub_safeTmp_ai_backend"/_input_hash.txt >> "$inference_cache_dir"/inference_cache_sha512_xz.txt.tmp-"$current_sub_sessionid_ai_backend".txt
        cat "$current_sub_safeTmp_ai_backend"/_output.txt | xz -z -e9 -C sha256 --threads=1 | head -c 32768 | base64 -w0 >> "$inference_cache_dir"/inference_cache_sha512_xz.txt.tmp-"$current_sub_sessionid_ai_backend".txt
        echo '' >> "$inference_cache_dir"/inference_cache_sha512_xz.txt.tmp-"$current_sub_sessionid_ai_backend".txt

        [[ -e "$inference_cache_dir"/"$current_sub_sessionid_ai_backend".lock ]] && mv -f "$inference_cache_dir"/inference_cache_sha512_xz.txt.tmp-"$current_sub_sessionid_ai_backend".txt "$inference_cache_dir"/inference_cache_sha512_xz.txt
        rm -f "$inference_cache_dir"/inference_cache_sha512_xz.txt.tmp-"$current_sub_sessionid_ai_backend".txt
        rm -f "$inference_cache_dir"/"$current_sub_sessionid_ai_backend".lock
    fi


    
    [[ "$inference_cache_dir" != "" ]] && rm -f "$inference_cache_dir"/inference_cache_sha512_xz.txt.tmp-"$current_sub_sessionid_ai_backend".txt
    [[ "$inference_cache_dir" != "" ]] && rm -f "$inference_cache_dir"/"$current_sub_sessionid_ai_backend".lock
    cat "$current_sub_safeTmp_ai_backend"/_output.txt
    _safeRMR "$current_sub_safeTmp_ai_backend"
    return 0
}






#echo please tell me a very short story | ./ubiquitous_bash.sh _vector_ai_backend 'models: [ "nvidia/nemotron-3-nano-30b-a3b", "openai/gpt-oss-120b" ], reasoning: { exclude: true }, provider: { "sort": "throughput" }' 'openrouter'

_vector_ai_backend_sequence() {
    _start

    _ai_backend_procedure "$@"

    _stop
}
_vector_ai_backend() {
    "$scriptAbsoluteLocation" _vector_ai_backend_sequence "$@"
}






_test_cloud_ai() {
    _wantGetDep curl
    _wantGetDep jq
    _wantGetDep perl

    _wantGetDep grep
    _wantGetDep tr
}
